# Test Specifications for Manufacturer Role Implementation - Layer 6

## ðŸŽ¯ Executive Summary

This document provides comprehensive test specifications for the manufacturer role implementation, addressing critical issues identified in the TDD Implementation Summary LS5 and reflection_LS4.md. The specifications focus on eliminating mock implementations, ensuring real database operations, and implementing robust security for business-critical manufacturer data.

**Key Focus Areas:**
1. Security testing for manufacturer authentication and data protection
2. Performance testing for centralized product repository operations  
3. Conversion tracking and agentic flow testing
4. Integration testing between manufacturer systems and existing eyewear ML tools
5. Dashboard access control and feature gating tests

---

## ðŸ”´ Critical Issues to Address

### Issue 1: Mock Implementation Elimination
**Problem**: Current implementation relies heavily on mock objects (CacheManager, AuditLogger, SecurityValidator)
**Impact**: False confidence in performance and security claims
**Test Strategy**: Comprehensive real implementation validation

### Issue 2: Security Implementation Gaps
**Problem**: Security tests return hardcoded `True` values without real validation
**Impact**: Vulnerable to actual security threats
**Test Strategy**: Real threat detection and validation testing

### Issue 3: Performance Claims Without Real Operations
**Problem**: 15,000+ ops/sec achieved through hash operations, not database operations
**Impact**: Misleading performance metrics
**Test Strategy**: Real database performance benchmarking

---

## ðŸ§ª Test Categories and Specifications

### 1. Security Testing for Manufacturer Authentication and Data Protection

#### 1.1 Manufacturer Authentication Tests

```python
class TestManufacturerAuthentication:
    """Test suite for manufacturer authentication and session management"""
    
    @pytest.mark.security
    @pytest.mark.asyncio
    async def test_manufacturer_jwt_token_validation(self, real_auth_manager):
        """Test real JWT token validation for manufacturer sessions"""
        # RED PHASE: Should fail until real JWT implementation exists
        manufacturer_data = {
            "company_name": "Test Eyewear Co",
            "email": "admin@testeyewear.com",
            "tier": "free"
        }
        
        # Generate real JWT token
        token = await real_auth_manager.generate_manufacturer_token(manufacturer_data)
        
        # Validate token structure and claims
        assert token is not None
        assert isinstance(token, str)
        assert len(token.split('.')) == 3  # JWT structure: header.payload.signature
        
        # Validate token claims
        claims = await real_auth_manager.validate_token(token)
        assert claims["company_name"] == manufacturer_data["company_name"]
        assert claims["email"] == manufacturer_data["email"]
        assert claims["tier"] == manufacturer_data["tier"]
        assert "exp" in claims  # Expiration claim
        assert "iat" in claims  # Issued at claim
        
        # Test token expiration
        expired_token = await real_auth_manager.generate_expired_token(manufacturer_data)
        with pytest.raises(TokenExpiredError):
            await real_auth_manager.validate_token(expired_token)
    
    @pytest.mark.security
    @pytest.mark.asyncio
    async def test_manufacturer_rbac_implementation(self, real_rbac_manager):
        """Test real RBAC implementation for manufacturer roles"""
        # Test free tier manufacturer permissions
        free_manufacturer = ManufacturerContext(
            manufacturer_id="mfg_001",
            company_name="Free Eyewear Co",
            tier="free",
            roles=[Role.MANUFACTURER_FREE]
        )
        
        # Free tier should have limited permissions
        assert real_rbac_manager.has_permission(free_manufacturer, Permission.UPLOAD_PRODUCTS)
        assert real_rbac_manager.has_permission(free_manufacturer, Permission.VIEW_BASIC_ANALYTICS)
        assert not real_rbac_manager.has_permission(free_manufacturer, Permission.ACCESS_ML_TOOLS)
        assert not real_rbac_manager.has_permission(free_manufacturer, Permission.EXPORT_DATA)
        assert not real_rbac_manager.has_permission(free_manufacturer, Permission.API_ACCESS)
        
        # Test paid tier manufacturer permissions
        paid_manufacturer = ManufacturerContext(
            manufacturer_id="mfg_002", 
            company_name="Premium Eyewear Co",
            tier="paid",
            roles=[Role.MANUFACTURER_PAID]
        )
        
        # Paid tier should have full permissions
        assert real_rbac_manager.has_permission(paid_manufacturer, Permission.UPLOAD_PRODUCTS)
        assert real_rbac_manager.has_permission(paid_manufacturer, Permission.VIEW_BASIC_ANALYTICS)
        assert real_rbac_manager.has_permission(paid_manufacturer, Permission.ACCESS_ML_TOOLS)
        assert real_rbac_manager.has_permission(paid_manufacturer, Permission.EXPORT_DATA)
        assert real_rbac_manager.has_permission(paid_manufacturer, Permission.API_ACCESS)
        assert real_rbac_manager.has_permission(paid_manufacturer, Permission.ADVANCED_ANALYTICS)
    
    @pytest.mark.security
    @pytest.mark.asyncio
    async def test_manufacturer_data_encryption(self, real_encryption_manager):
        """Test encryption of sensitive manufacturer business data"""
        sensitive_data = {
            "business_license": "BL123456789",
            "tax_id": "TAX987654321",
            "bank_account": "ACC555666777",
            "contact_phone": "+1-555-123-4567",
            "revenue_data": {"annual_revenue": 5000000}
        }
        
        # Test encryption
        encrypted_data = await real_encryption_manager.encrypt_manufacturer_data(sensitive_data)
        assert encrypted_data != sensitive_data
        assert "business_license" not in str(encrypted_data)
        assert "tax_id" not in str(encrypted_data)
        
        # Test decryption
        decrypted_data = await real_encryption_manager.decrypt_manufacturer_data(encrypted_data)
        assert decrypted_data == sensitive_data
        
        # Test encryption key rotation
        rotated_key_data = await real_encryption_manager.rotate_encryption_key(encrypted_data)
        re_decrypted_data = await real_encryption_manager.decrypt_manufacturer_data(rotated_key_data)
        assert re_decrypted_data == sensitive_data
```

#### 1.2 Input Validation and Threat Detection Tests

```python
class TestManufacturerSecurityValidation:
    """Test suite for manufacturer input validation and threat detection"""
    
    @pytest.mark.security
    @pytest.mark.asyncio
    async def test_real_input_validation_patterns(self, real_security_validator):
        """Test real input validation for manufacturer data uploads"""
        # Test SQL injection attempts
        malicious_inputs = [
            "'; DROP TABLE products; --",
            "' OR '1'='1",
            "UNION SELECT * FROM users",
            "'; INSERT INTO products VALUES ('malicious'); --"
        ]
        
        for malicious_input in malicious_inputs:
            product_data = {
                "name": malicious_input,
                "description": "Test product",
                "sku": "TEST123"
            }
            
            is_threat = await real_security_validator.detect_threat("sql_injection", product_data)
            assert is_threat == True, f"Failed to detect SQL injection: {malicious_input}"
    
    @pytest.mark.security
    @pytest.mark.asyncio
    async def test_nosql_injection_detection(self, real_security_validator):
        """Test NoSQL injection detection for MongoDB operations"""
        nosql_payloads = [
            {"$where": "this.name == 'admin'"},
            {"$ne": None},
            {"$regex": ".*"},
            {"$gt": ""},
            {"name": {"$ne": None}}
        ]
        
        for payload in nosql_payloads:
            is_threat = await real_security_validator.detect_threat("nosql_injection", payload)
            assert is_threat == True, f"Failed to detect NoSQL injection: {payload}"
    
    @pytest.mark.security
    @pytest.mark.asyncio
    async def test_business_data_validation(self, real_data_validator):
        """Test validation of business-critical manufacturer data"""
        # Test valid business data
        valid_data = {
            "company_name": "Valid Eyewear Company",
            "business_email": "contact@valideyewear.com",
            "business_license": "BL123456789",
            "tax_id": "12-3456789",
            "website": "https://valideyewear.com"
        }
        
        validation_result = await real_data_validator.validate_manufacturer_data(valid_data)
        assert validation_result.is_valid == True
        assert len(validation_result.errors) == 0
        
        # Test invalid business data
        invalid_data = {
            "company_name": "",  # Empty name
            "business_email": "invalid-email",  # Invalid email format
            "business_license": "123",  # Too short
            "tax_id": "invalid-tax-id",  # Invalid format
            "website": "not-a-url"  # Invalid URL
        }
        
        validation_result = await real_data_validator.validate_manufacturer_data(invalid_data)
        assert validation_result.is_valid == False
        assert len(validation_result.errors) == 5  # All fields should have errors
```

### 2. Performance Testing for Centralized Product Repository Operations

#### 2.1 Real Database Performance Tests

```python
class TestProductRepositoryPerformance:
    """Test suite for real database performance benchmarking"""
    
    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_real_product_upload_performance(self, real_product_manager, performance_monitor):
        """Test real product upload performance with MongoDB operations"""
        # Generate realistic product data
        products = []
        for i in range(1000):
            products.append({
                "sku": f"TEST-SKU-{i:06d}",
                "name": f"Test Product {i}",
                "brand": "Test Brand",
                "manufacturer_id": "mfg_001",
                "price": 99.99 + (i * 0.01),
                "description": f"Test product description for item {i}",
                "category": "sunglasses",
                "frame_material": "acetate",
                "lens_material": "polycarbonate",
                "dimensions": {
                    "lens_width": 52 + (i % 10),
                    "bridge_width": 18 + (i % 5),
                    "temple_length": 140 + (i % 15)
                }
            })
        
        # Test bulk upload performance
        start_time = time.perf_counter()
        results = await real_product_manager.bulk_upload_products(products)
        end_time = time.perf_counter()
        
        upload_time = end_time - start_time
        products_per_second = len(products) / upload_time
        
        # Performance assertions
        assert upload_time < 30.0, f"Bulk upload took {upload_time:.2f}s, expected < 30s"
        assert products_per_second > 30, f"Upload rate {products_per_second:.2f} products/s, expected > 30/s"
        assert len(results.successful) == len(products), "Not all products uploaded successfully"
        assert len(results.failed) == 0, f"Failed uploads: {results.failed}"
        
        # Verify database state
        total_products = await real_product_manager.count_products_by_manufacturer("mfg_001")
        assert total_products >= len(products)
    
    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_real_product_search_performance(self, real_search_manager, real_product_manager):
        """Test real product search performance with complex queries"""
        # Setup: Create searchable products
        await self._setup_search_test_data(real_product_manager)
        
        # Test simple search performance
        start_time = time.perf_counter()
        simple_results = await real_search_manager.search_products(
            query="sunglasses",
            manufacturer_id="mfg_001"
        )
        simple_time = time.perf_counter() - start_time
        
        assert simple_time < 0.1, f"Simple search took {simple_time:.3f}s, expected < 0.1s"
        assert len(simple_results) > 0, "Simple search returned no results"
        
        # Test complex search performance
        start_time = time.perf_counter()
        complex_results = await real_search_manager.search_products(
            query="acetate sunglasses",
            filters={
                "price_range": {"min": 50, "max": 200},
                "frame_material": "acetate",
                "lens_width_range": {"min": 50, "max": 60}
            },
            manufacturer_id="mfg_001",
            sort_by="price",
            limit=50
        )
        complex_time = time.perf_counter() - start_time
        
        assert complex_time < 0.2, f"Complex search took {complex_time:.3f}s, expected < 0.2s"
        assert len(complex_results) > 0, "Complex search returned no results"
        
        # Test search with aggregations
        start_time = time.perf_counter()
        aggregated_results = await real_search_manager.search_with_aggregations(
            query="eyeglasses",
            aggregations=["brand", "frame_material", "price_range"],
            manufacturer_id="mfg_001"
        )
        aggregation_time = time.perf_counter() - start_time
        
        assert aggregation_time < 0.3, f"Aggregated search took {aggregation_time:.3f}s, expected < 0.3s"
        assert "aggregations" in aggregated_results
        assert len(aggregated_results["products"]) > 0
    
    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_real_caching_performance(self, real_cache_manager):
        """Test real caching implementation performance"""
        # Test cache miss performance
        cache_key = "manufacturer:mfg_001:products:page_1"
        
        start_time = time.perf_counter()
        cache_miss_result = await real_cache_manager.get(cache_key)
        cache_miss_time = time.perf_counter() - start_time
        
        assert cache_miss_result is None
        assert cache_miss_time < 0.001, f"Cache miss took {cache_miss_time:.4f}s, expected < 0.001s"
        
        # Test cache set performance
        test_data = {"products": [{"id": i, "name": f"Product {i}"} for i in range(100)]}
        
        start_time = time.perf_counter()
        await real_cache_manager.set(cache_key, test_data, ttl=300)
        cache_set_time = time.perf_counter() - start_time
        
        assert cache_set_time < 0.01, f"Cache set took {cache_set_time:.4f}s, expected < 0.01s"
        
        # Test cache hit performance
        start_time = time.perf_counter()
        cache_hit_result = await real_cache_manager.get(cache_key)
        cache_hit_time = time.perf_counter() - start_time
        
        assert cache_hit_result is not None
        assert cache_hit_result == test_data
        assert cache_hit_time < 0.001, f"Cache hit took {cache_hit_time:.4f}s, expected < 0.001s"
        
        # Test cache invalidation performance
        start_time = time.perf_counter()
        await real_cache_manager.invalidate_pattern("manufacturer:mfg_001:*")
        invalidation_time = time.perf_counter() - start_time
        
        assert invalidation_time < 0.01, f"Cache invalidation took {invalidation_time:.4f}s, expected < 0.01s"
        
        # Verify invalidation worked
        invalidated_result = await real_cache_manager.get(cache_key)
        assert invalidated_result is None
```

### 3. Conversion Tracking and Agentic Flow Testing

#### 3.1 Agentic Onboarding Flow Tests

```python
class TestAgenticOnboardingFlow:
    """Test suite for agentic manufacturer onboarding and conversion tracking"""
    
    @pytest.mark.agentic
    @pytest.mark.asyncio
    async def test_personalized_onboarding_flow(self, real_onboarding_manager):
        """Test personalized onboarding flow based on manufacturer profile"""
        # Test small manufacturer onboarding
        small_manufacturer_profile = {
            "company_size": "small",
            "annual_revenue": 500000,
            "product_count_estimate": 50,
            "primary_market": "local",
            "tech_savviness": "medium"
        }
        
        onboarding_flow = await real_onboarding_manager.generate_personalized_flow(
            small_manufacturer_profile
        )
        
        # Verify personalized flow structure
        assert onboarding_flow.total_steps <= 5, "Small manufacturer flow should be simplified"
        assert "product_upload_tutorial" in onboarding_flow.step_types
        assert "basic_analytics_demo" in onboarding_flow.step_types
        assert "ml_tools_preview" in onboarding_flow.step_types  # Conversion hook
        
        # Test large manufacturer onboarding
        large_manufacturer_profile = {
            "company_size": "large",
            "annual_revenue": 50000000,
            "product_count_estimate": 5000,
            "primary_market": "international",
            "tech_savviness": "high"
        }
        
        large_onboarding_flow = await real_onboarding_manager.generate_personalized_flow(
            large_manufacturer_profile
        )
        
        # Verify enterprise-focused flow
        assert large_onboarding_flow.total_steps <= 8
        assert "bulk_upload_tutorial" in large_onboarding_flow.step_types
        assert "api_integration_demo" in large_onboarding_flow.step_types
        assert "enterprise_features_preview" in large_onboarding_flow.step_types
    
    @pytest.mark.agentic
    @pytest.mark.asyncio
    async def test_conversion_tracking_accuracy(self, real_analytics_manager):
        """Test conversion tracking accuracy and attribution"""
        # Simulate manufacturer journey
        manufacturer_id = "mfg_test_001"
        
        # Track onboarding events
        await real_analytics_manager.track_event(manufacturer_id, "onboarding_started", {
            "source": "organic_search",
            "landing_page": "/manufacturer-signup"
        })
        
        await real_analytics_manager.track_event(manufacturer_id, "profile_completed", {
            "completion_time": 120  # seconds
        })
        
        await real_analytics_manager.track_event(manufacturer_id, "first_product_uploaded", {
            "product_count": 1,
            "upload_method": "manual"
        })
        
        # Track conversion trigger events
        await real_analytics_manager.track_event(manufacturer_id, "ml_tools_preview_viewed", {
            "feature": "face_shape_compatibility",
            "engagement_time": 45
        })
        
        await real_analytics_manager.track_event(manufacturer_id, "upgrade_prompt_shown", {
            "trigger": "ml_tools_limit_reached",
            "prompt_type": "modal"
        })
        
        await real_analytics_manager.track_event(manufacturer_id, "upgrade_completed", {
            "plan": "professional",
            "payment_method": "credit_card"
        })
        
        # Analyze conversion funnel
        funnel_analysis = await real_analytics_manager.analyze_conversion_funnel(manufacturer_id)
        
        # Verify tracking accuracy
        assert funnel_analysis.total_events == 6
        assert funnel_analysis.conversion_achieved == True
        assert funnel_analysis.time_to_conversion <= 3600  # Within 1 hour
        assert funnel_analysis.conversion_trigger == "ml_tools_limit_reached"
        
        # Test attribution accuracy
        attribution = await real_analytics_manager.get_conversion_attribution(manufacturer_id)
        assert attribution.first_touch_source == "organic_search"
        assert attribution.last_touch_trigger == "ml_tools_limit_reached"
        assert attribution.total_touchpoints == 6
    
    @pytest.mark.agentic
    @pytest.mark.asyncio
    async def test_intelligent_upgrade_prompts(self, real_prompt_manager):
        """Test intelligent upgrade prompt timing and personalization"""
        manufacturer_context = {
            "id": "mfg_test_002",
            "tier": "free",
            "days_since_signup": 7,
            "products_uploaded": 25,
            "ml_tool_usage": 15,  # Approaching limit
            "engagement_score": 0.8
        }
        
        # Test prompt generation
        upgrade_prompt = await real_prompt_manager.generate_upgrade_prompt(manufacturer_context)
        
        # Verify prompt personalization
        assert upgrade_prompt is not None
        assert upgrade_prompt.trigger_reason == "approaching_ml_limit"
        assert upgrade_prompt.personalization_score > 0.7
        assert "25 products" in upgrade_prompt.message  # Personalized content
        assert upgrade_prompt.urgency_level == "medium"
        
        # Test prompt timing optimization
        optimal_timing = await real_prompt_manager.calculate_optimal_timing(manufacturer_context)
        assert optimal_timing.should_show_now == True
        assert optimal_timing.confidence_score > 0.6
        
        # Test A/B testing capability
        ab_variant = await real_prompt_manager.get_ab_test_variant(manufacturer_context["id"])
        assert ab_variant in ["control", "variant_a", "variant_b"]
        
        # Track prompt effectiveness
        await real_prompt_manager.track_prompt_interaction(
            manufacturer_context["id"],
            upgrade_prompt.prompt_id,
            "clicked"
        )
        
        effectiveness = await real_prompt_manager.get_prompt_effectiveness(upgrade_prompt.prompt_id)
        assert effectiveness.total_impressions >= 1
        assert effectiveness.click_rate >= 0.0
```

### 4. Integration Testing Between Manufacturer Systems and Existing Eyewear ML Tools

#### 4.1 ML Tools Integration Tests

```python
class TestManufacturerMLIntegration:
    """Test suite for manufacturer integration with eyewear ML tools"""
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_face_shape_compatibility_integration(self, real_ml_service, real_product_manager):
        """Test integration between manufacturer products and face shape ML tools"""
        # Setup manufacturer product
        manufacturer_id = "mfg_ml_test_001"
        product_data = {
            "sku": "ML-TEST-001",
            "name": "Test Aviator Sunglasses",
            "manufacturer_id": manufacturer_id,
            "dimensions": {
                "lens_width": 58,
                "bridge_width": 14,
                "temple_length": 140,
                "frame_width": 130,
                "frame_height": 45
            },
            "frame_shape": "aviator",
            "frame_material": "metal"
        }
        
        created_product = await real_product_manager.create_product(product_data)
        
        # Test ML analysis integration
        ml_analysis = await real_ml_service.analyze_product_compatibility(created_product.id)
        
        # Verify ML analysis results
        assert ml_analysis is not None
        assert "face_shape_compatibility" in ml_analysis
        assert "style_recommendations" in ml_analysis
        assert ml_analysis["face_shape_compatibility"]["oval"] > 0.7  # Aviators suit oval faces
        assert ml_analysis["face_shape_compatibility"]["square"] > 0.6  # Also good for square faces
        
        # Test batch ML processing for manufacturer
        manufacturer_products = await real_product_manager.get_products_by_manufacturer(manufacturer_id)
        batch_analysis = await real_ml_service.batch_analyze_manufacturer_products(manufacturer_id)
        
        assert len(batch_analysis) == len(manufacturer_products)
        assert all("face_shape_compatibility" in analysis for analysis in batch_analysis.values())
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_style_matching_integration(self, real_style_service, real_product_manager):
        """Test integration with style matching ML tools"""
        manufacturer_id = "mfg_style_test_001"
        
        # Create diverse product portfolio
        products = [
            {
                "sku": "STYLE-001",
                "name": "Classic Wayfarer",
                "style_tags": ["classic", "casual", "retro"],
                "frame_shape": "wayfarer",
                "manufacturer_id": manufacturer_id
            },
            {
                "sku": "STYLE-002", 
                "name": "Modern Cat Eye",
                "style_tags": ["modern", "feminine", "bold"],
                "frame_shape": "cat_eye",
                "manufacturer_id": manufacturer_id
            },
            {
                "sku": "STYLE-003",
                "name": "Professional Rectangle",
                "style_tags": ["professional", "minimal", "business"],
                "frame_shape": "rectangle",
                "manufacturer_id": manufacturer_id
            }
        ]
        
        created_products = []
        for product_data in products:
            created_product = await real_product_manager.create_product(product_data)
            created_products.append(created_product)
        
        # Test style analysis
        for product in created_products:
            style_analysis = await real_style_service.analyze_product_style(product.id)
            
            assert style_analysis is not None
            assert "style_score" in style_analysis
            assert "target_demographics" in style_analysis
            assert "occasion_suitability" in style_analysis
            assert style_analysis["style_score"] > 0.5
        
        # Test style-based recommendations
        user_style_profile = {
            "preferred_styles": ["classic", "professional"],
            "face_shape": "oval",
            "age_range": "25-35",
            "gender": "unisex"
        }
        
        recommendations = await real_style_service.get_style_recommendations(
            user_style_profile,
            manufacturer_filter=manufacturer_id
        )
        
        assert len(recommendations) > 0
        assert any(rec["sku"] in ["STYLE-001", "STYLE-003"] for rec in recommendations)
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_virtual_try_on_integration(self, real_vto_service, real_product_manager):
        """Test integration with virtual try-on ML tools"""
        manufacturer_id = "mfg_vto_test_001"
        
        # Create product with VTO-compatible data
        product_data = {
            "sku": "VTO-TEST-001",
            "name": "VTO Test Glasses",
            "manufacturer_id": manufacturer_id,
            "dimensions": {
                "lens_width": 52,
                "bridge_width": 18,
                "temple_length": 145,
                "frame_width": 135,
                "frame_height": 40
            },
            "frame_shape": "round",
            "frame_color": "black",
            "lens_color": "clear",
            "3d_model_url": "https://example.com/models/vto-test-001.glb"
        }
        
        created_product = await real_product_manager.create_product(product_data)
        
        # Test VTO model generation
        vto_model = await real_vto_service.generate_vto_model(created_product.id)
        
        assert vto_model is not None
        assert "model_id" in vto_model
        assert "calibration_points" in vto_model
        assert len(vto_model["calibration_points"]) >= 8  # Minimum points for accurate fitting
        
        # Test VTO fitting simulation
        user_face_data = {
            "face_width": 140,
            "face_height": 180,
            "interpupillary_distance": 62,
            "nose_bridge_width": 16
        }
        
        fitting_result = await real_vto_service.simulate_fitting(
            vto_model["model_id"],
            user_face_data
        )
        
        assert fitting_result is not None
        assert "fit_score" in fitting_result
        assert "adjustment_recommendations" in fitting_result
        assert fitting_result["fit_score"] > 0.0
```

### 5. Dashboard Access Control and Feature Gating Tests

#### 5.1 Tiered Access Control Tests

```python
class TestManufacturerDashboardAccess:
    """Test suite for manufacturer dashboard access control and feature gating"""
    
    @pytest.mark.dashboard
    @pytest.mark.asyncio
    async def test_free_tier_feature_access(self, real_dashboard_manager, free_manufacturer_context):
        """Test feature access for free tier manufacturers"""
        # Test accessible features for free tier
        accessible_features = await real_dashboard_manager.get_accessible_features(
            free_manufacturer_context
        )
        
        # Free tier should have basic features
        assert "product_upload" in accessible_features
        assert "basic_analytics" in accessible_features
        assert "product_listing" in accessible_features
        assert "profile_management" in accessible_features
        
        # Free tier should NOT have premium features
        assert "ml_tools_access" not in accessible_features
        assert "api_access" not in accessible_features
        assert "advanced_analytics" not in accessible_features
        assert "bulk_export" not in accessible_features
        assert "white_label_options" not in accessible_features
        
        # Test feature usage limits for free tier
        usage_limits = await real_dashboard_manager.get_usage_limits(free_manufacturer_context)
        
        assert usage_limits["product_uploads"]["monthly_limit"] == 100
        assert usage_limits["ml_tool_requests"]["monthly_limit"] == 50
        assert usage_limits["api_calls"]["monthly_limit"] == 0  # No API access
        assert usage_limits["data_export"]["monthly_limit"] == 1
    
    @pytest.mark.dashboard
    @pytest.mark.asyncio
    async def test_paid_tier_feature_access(self, real_dashboard_manager, paid_manufacturer_context):
        """Test feature access for paid tier manufacturers"""
        # Test accessible features for paid tier
        accessible_features = await real_dashboard_manager.get_accessible_features(
            paid_manufacturer_context
        )
        
        # Paid tier should have all features
        assert "product_upload" in accessible_features
        assert "basic_analytics" in accessible_features
        assert "advanced_analytics" in accessible_features
        assert "ml_tools_access" in accessible_features
        assert "api_access" in accessible_features
        assert "bulk_export" in accessible_features
        assert "white_label_options" in accessible_features
        assert "priority_support" in accessible_features
        
        # Test enhanced usage limits for paid tier
        usage_limits = await real_dashboard_manager.get_usage_limits(paid_manufacturer_context)
        
        assert usage_limits["product_uploads"]["monthly_limit"] == 10000
        assert usage_limits["ml_tool_requests"]["monthly_limit"] == 5000
        assert usage_limits["api_calls"]["monthly_limit"] == 100000
        assert usage_limits["data_export"]["monthly_limit"] == -1  # Unlimited
    
    @pytest.mark.dashboard
    @pytest.mark.asyncio
    async def test_feature_gating_enforcement(self, real_feature_gate_manager):
        """Test enforcement of feature gating based on manufacturer tier"""
        free_manufacturer = ManufacturerContext(
            manufacturer_id="mfg_gate_test_001",
            tier="free",
            current_usage={
                "ml_tool_requests": 45,  # Near limit
                "product_uploads": 95    # Near limit
            }
        )
        
        # Test ML tools access gating
        ml_access_result = await real_feature_gate_manager.check_feature_access(
            free_manufacturer,
            "ml_tools_access",
            requested_usage=10
        )
        
        assert ml_access_result.allowed == False
        assert ml_access_result.reason == "feature_not_available_in_tier"
        assert "upgrade" in ml_access_result.upgrade_prompt.lower()
        
        # Test usage limit enforcement
        upload_access_result = await real_feature_gate_manager.check_feature_access(
            free_manufacturer,
            "product_upload",
            requested_usage=10  # Would exceed limit
        )
        
        assert upload_access_result.allowed == False
        assert upload_access_result.reason == "usage_limit_exceeded"
        assert upload_access_result.current_usage == 95
        assert upload_access_result.limit == 100
        
        # Test successful access within limits
        small_upload_result = await real_feature_gate_manager.check_feature_access(
            free_manufacturer,
            "product_upload",
            requested_usage=3  # Within remaining limit
        )
        
        assert small_upload_result.allowed == True
        assert small_upload_result.remaining_usage == 2  # 100 - 95 - 3
    
    @pytest.mark.dashboard
    @pytest.mark.asyncio
    async def test_progressive_feature_disclosure(self, real_disclosure_manager):
        """Test progressive feature disclosure to encourage upgrades"""
        manufacturer_context = ManufacturerContext(
            manufacturer_id="mfg_disclosure_test_001",
            tier="free",
            engagement_metrics={
                "days_active": 14,
                "products_uploaded": 50,
                "dashboard_visits": 25,
                "feature_exploration_score": 0.7
            }
        )
        
        # Test feature preview generation
        feature_previews = await real_disclosure_manager.generate_feature_previews(
            manufacturer_context
        )
        
        # Should show relevant premium features based on usage
        assert len(feature_previews) > 0
        assert any(preview["feature"] == "ml_tools_access" for preview in feature_previews)
        assert any(preview["feature"] == "advanced_analytics" for preview in feature_previews)
        
        # Test preview content quality
        ml_preview = next(p for p in feature_previews if p["feature"] == "ml_tools_access")
        assert "value_proposition" in ml_preview
        assert "demo_available" in ml_preview
        assert ml_preview["relevance_score"] > 0.6
        
        # Test progressive disclosure timing
        disclosure_timing = await real_disclosure_manager.calculate_optimal_disclosure_timing(
            manufacturer_context
        )
        
        assert disclosure_timing.should_show_advanced_features == True
        assert disclosure_timing.readiness_score > 0.5
        assert "ml_tools" in disclosure_timing.recommended_features
```

---

## ðŸŽ¯ Test Fixtures and Infrastructure

### Required Test Fixtures

```python
# Real MongoDB Test Container
@pytest.fixture(scope="session")
async def mongodb_container():
    """Real MongoDB test container - NO MOCKS"""
    container = await start_mongodb_test_container()
    yield container
    await container.cleanup()

# Real Manufacturer Manager
@pytest.fixture
async def real_manufacturer_manager(mongodb_container):
    """Real ManufacturerManager with actual MongoDB operations"""
    db = await mongodb_container.get_database()
    manager = ManufacturerManager(db)
    await manager.initialize()
    return manager

# Real Security Components
@pytest.fixture
async def real_security_validator():
    """Real SecurityValidator with actual threat detection"""
    validator = SecurityValidator()
    await validator.load_threat_patterns()
    return validator

@pytest.fixture
async def real_auth_manager():
    """Real AuthManager with JWT implementation"""
    auth_manager = AuthManager(
        secret_key=os.environ.get("JWT_SECRET_KEY"),
        algorithm="HS256"
    )
    return auth_manager

# Real Performance Monitoring
@pytest.fixture
async def performance_monitor():
    """Real PerformanceMonitor for actual timing measurements"""
    monitor = PerformanceMonitor()
    await monitor.initialize()
    return monitor
```

---

## ðŸ“Š Success Criteria and Quality Gates

### Test Coverage Requirements
- **Security Tests**: 100% coverage of authentication, authorization, and data protection
- **Performance Tests**: Real database operations with SLA validation
- **Integration Tests**: End-to-end manufacturer workflow validation
- **Feature Gating Tests**: Complete tier-based access control validation

### Performance Benchmarks
- **Product Upload**: <30s for 1000 products
- **Search Operations**: <100ms for complex queries
- **Dashboard Load**: <3s for full dashboard
- **Cache Operations**: <1ms for hit/miss operations

### Security Standards
- **Authentication**: Real JWT validation with proper expiration
- **Authorization**: RBAC with manufacturer-specific permissions
- **Data Protection**: Encryption for sensitive business data
- **Threat Detection**: Real pattern matching for security threats

### Quality Gates
1. **Zero Mock Dependencies**: All tests must use real implementations
2. **Real Database Operations**: All data operations must use actual MongoDB
3. **Actual Security Validation**: No hardcoded security responses
4. **Performance SLA Compliance**: All operations must meet timing requirements
5. **Comprehensive Error Handling**: All failure scenarios must be tested

---

## ðŸš€ Implementation Roadmap

### Phase 1: Foundation (Week 1)
- Implement real MongoDB test containers
- Create manufacturer authentication system
- Build basic RBAC for manufacturer roles

### Phase 2: Core Features (Week 2)
- Implement product repository with real database operations
- Create manufacturer dashboard with tiered access
- Build performance monitoring and caching

### Phase 3: Advanced Features (Week 3)
- Implement ML tools integration
- Create agentic onboarding flows
- Build conversion tracking system

### Phase 4: Security & Performance (Week 4)
- Implement comprehensive security validation
- Optimize performance for industry-scale operations
- Complete integration testing

### Phase 5: Validation & Deployment (Week 5)
- Run comprehensive test suite
- Validate all quality gates
- Deploy to production environment

This comprehensive test specification addresses all critical issues identified in the reflection analysis while ensuring the manufacturer role implementation meets production-ready standards with real database operations, robust security, and measurable conversion optimization.